{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.在CTR点击率预估中，使用GBDT+LR的原理是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR适合处理简单的线性特征，当需要挖掘更深层次的特征时，对人力和时间的成本要求很高。这时候就可以使用GBDT来进行特征的组合，即GBDT中一个叶子节点天然可以代表一种特征选择的路径，将叶子节点进行编码后就可以作为新的特征传到LR进行模型的构建。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Wide & Deep的模型结构是怎样的，为什么能通过具备记忆和泛化能力（memorization and generalization）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wide部分指的是LR，Deep指的是FNN模型，使用FM进行参数初始化。记忆能力是LR提供的，可以学习特征之间的相关频率，在历史数据中探索相关性的可能性。泛化能力是Deep部分提供，基于相关性的传递，去探索一些过去没有出现过的特征组合。同时训练Wide模型和Deep模型，并将两个模型的结果的加权作为最终的预测结果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.在CTR预估中，使用FM与DNN结合的方式，有哪些结合的方式，代表模型有哪些？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1）并行  代表模型DeepFM, FM做特征组合介于计算量的原因一般只考虑2阶特征组合，而更高阶的特征组合就交给DNN去做。FM和DNN同时执行。\n",
    "### （2）串行  代表模型NFM。对embedding直接采用对位相乘后相加起来作为交叉特征，然后通过DNN直接将特征压缩，最后concatenate linear部分和deep部分的特征。即将FM的结果作为DNN的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Surprise工具中的baseline算法原理是怎样的？BaselineOnly和KNNBaseline有什么区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline原理：基于统计的基准预测线打分，不仅考虑分值本身的一个平均值，还考虑物品对整体的偏差以及用户打分习惯对与整体的偏差。\n",
    "### KNNBaseline考虑了邻域的个数，即固定数量的邻居k。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.GBDT和随机森林都是基于树的算法，它们有什么区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT是基于残差的学习，每次对残差进行一次学习就会生成一颗决策树，是一种串行的方式；而随机森林是若干个树模型一起学习，然后再根据学习的结果来生成最终的结果，可以是选举的形式等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.基于邻域的协同过滤都有哪些算法，请简述原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNNWithMeans,KNNBaseline,KNNWithZScore,KNNBasic\n",
    "### KNNBasic是用用户对物品的兴趣来做就算，KNNWithMeans是将用户对物品的兴趣减去用户的平均兴趣来减少用户本身的特性带来的影响；KNNWithZScore是使用标准差的方式对其进行了归一化处理；KNNBaseline是使用了Baseline算法，考虑了用户打分的偏差和物品本身的偏差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
